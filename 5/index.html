<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Project 5A — Diffusion Models (CS180)</title>
  <meta name="description" content="Part A: Using pretrained DeepFloyd diffusion models for image generation and manipulation." />
  <style>
    :root{
      --bg:#f5f9ff; --ink:#0e1b2c; --muted:#5a6b82;
      --accent:#3b82f6; --accent-2:#60a5fa;
      --card:#ffffff; --border:#e6edf7;
      --shadow:0 10px 25px rgba(25,74,142,.12);
      --radius:18px; --max:1080px;
    }
    html,body{background:var(--bg); color:var(--ink); font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, "Helvetica Neue", Arial}
    body{margin:0; padding:28px 18px 56px}
    .wrap{max-width:var(--max); margin:0 auto}
    a{color:var(--accent); text-decoration:none}
    header{margin-bottom:1rem}
    h1{font-size:clamp(1.6rem, 1.15rem + 2vw, 2.4rem); line-height:1.15; margin:.25rem 0}
    .pill{display:inline-block; padding:.25rem .6rem; border-radius:999px; border:1px solid rgba(59,130,246,.25); background:rgba(59,130,246,.12); color:#0b3a7a; font-weight:600; font-size:.85rem}

    .card{background:var(--card); border:1px solid var(--border); border-radius:var(--radius); padding:18px; box-shadow:var(--shadow); margin:18px 0}
    .grid{display:grid; gap:14px}
    .two{grid-template-columns:1fr}
    .three{grid-template-columns:1fr}
    @media(min-width:780px){ .two{grid-template-columns:1fr 1fr} .three{grid-template-columns:1fr 1fr 1fr} }

    figure{margin:0}
    img{width:100%; height:auto; display:block; border-radius:14px; border:1px solid var(--border)}
    figcaption{margin-top:.4rem; color:var(--muted); font-size:.95rem}
    h2{margin:.25rem 0 .65rem; font-size:1.3rem}
    h3{margin:.25rem 0 .4rem; font-size:1.05rem}
    .hr{height:1px; background:var(--border); margin:1.5rem 0}
    footer{margin-top:2rem; color:var(--muted); font-size:.95rem}
    pre{background:#f3f6ff; border:1px solid #e6edf7; padding:.6rem .75rem; border-radius:10px; overflow:auto; font-size:.92rem; line-height:1.35}
    code{background:#eef4ff; border:1px solid #d7e5ff; padding:.1rem .25rem; border-radius:6px}
    .part-header{background:linear-gradient(135deg, #3b82f6 0%, #60a5fa 100%); color:#fff; padding:1rem 1.25rem; border-radius:var(--radius); margin:2rem 0 1rem}
    .part-header h2{color:#fff; margin:0}
    @media print{ body{padding:0; background:#fff} .card{box-shadow:none} a[href]:after{content:""} }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <a href="/">← Back to Home</a>
      <h1>Programming Project #5A — The Power of Diffusion Models</h1>
      <span class="pill">CS180/280A</span>
      <p style="margin:.4rem 0 0; color:var(--muted)">
        Using pretrained DeepFloyd diffusion models for image generation, manipulation, and optical illusions.
      </p>
    </header>

    <!-- ========= Part 0 ========= -->
    <div class="part-header"><h2>Part 0 — Setup & Text Prompts</h2></div>

    <section class="card" id="part0">
      <h2>Generated Images from Text Prompts</h2>
      <p>
        Using DeepFloyd IF to generate images from custom text prompts. Testing with different
        <code>num_inference_steps</code> values to observe quality differences.
      </p>

      <figure>
        <img src="media/proj5_0_1.png" alt="Generated samples">
        <figcaption>
          Three generated samples: "an oil painting of a snowy mountain village", 
          "a lithograph of waterfalls", and "a rocket ship"
        </figcaption>
      </figure>

      <p style="margin-top:.8rem; color:var(--muted)">
        <strong>Random seed:</strong> 180<br>
        <strong>num_inference_steps tested:</strong> 20, 50
      </p>
    </section>

    <!-- ========= Part 1 ========= -->
    <div class="part-header"><h2>Part 1 — Sampling Loops</h2></div>

    <!-- ========= 1.1 ========= -->
    <section class="card" id="1-1">
      <h2>1.1 — Implementing the Forward Process</h2>
      <p>
        The forward diffusion process adds Gaussian noise to clean images according to:
        <code>x_t = √(ᾱ_t)·x_0 + √(1-ᾱ_t)·ε</code> where <code>ε ~ N(0, I)</code>.
      </p>

      <h3>Forward Process Equation</h3>
      <pre>
x_t = √(ᾱ_t) · x_0 + √(1-ᾱ_t) · ε

where:
  x_0 = clean image (Campanile)
  x_t = noisy image at timestep t
  ᾱ_t = noise schedule coefficient (alphas_cumprod[t])
  ε ~ N(0, I) = Gaussian noise
      </pre>

      <figure>
        <img src="media/proj5_1_1.png" alt="Forward process">
        <figcaption>
          Berkeley Campanile at different noise levels: Original, t=250, t=500, t=750
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.2 ========= -->
    <section class="card" id="1-2">
      <h2>1.2 — Classical Denoising with Gaussian Blur</h2>
      <p>
        Attempting to denoise using classical Gaussian blur filtering (kernel_size=9, sigma=2.0).
        Results demonstrate the limitations of traditional methods compared to learned diffusion models.
      </p>

      <figure>
        <img src="media/proj5_1_2.png" alt="Gaussian blur denoising">
        <figcaption>
          Top row: Noisy images at t=250, 500, 750<br>
          Bottom row: Gaussian blur denoised results (limited effectiveness)
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.3 ========= -->
    <section class="card" id="1-3">
      <h2>1.3 — One-Step Denoising</h2>
      <p>
        Using the pretrained UNet to estimate and remove noise in a single step:
        <code>x_0 = (x_t - √(1-ᾱ_t)·ε_θ(x_t, t)) / √(ᾱ_t)</code>
      </p>

      <figure>
        <img src="media/proj5_1_3.png" alt="One-step denoising">
        <figcaption>
          One-step denoising at t=250, 500, 750. Each triplet shows: Original, Noisy, One-step Denoised.
          Performance degrades at higher noise levels.
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.4 ========= -->
    <section class="card" id="1-4">
      <h2>1.4 — Iterative Denoising (DDPM)</h2>
      <p>
        Implementing the full iterative denoising process with strided timesteps: [990, 960, ..., 30, 0].
        Starting from i_start=10 (t=690), we progressively remove noise over multiple steps.
      </p>

      <h3>DDPM Algorithm</h3>
      <pre>
strided_timesteps = [990, 960, 930, ..., 30, 0]  (stride=30)

For each step i → i+1:
  1. Estimate noise: ε = UNet(x_t, t, prompt_embeds)
  2. Estimate clean image: x_0 = (x_t - √(1-ᾱ_t)·ε) / √(ᾱ_t)
  3. Step to next timestep using DDPM formula
  4. Add variance for stochasticity
      </pre>

      <figure>
        <img src="media/proj5_1_4.png" alt="Iterative denoising">
        <figcaption>
          Comparison of denoising methods: Original → Noisy (t=690) → Iterative DDPM → 
          One-step → Gaussian Blur. Iterative denoising produces the best results.
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.5 ========= -->
    <section class="card" id="1-5">
      <h2>1.5 — Diffusion Model Sampling</h2>
      <p>
        Generating images from pure noise by running iterative denoising from t=990 to t=0.
        Using prompt: "a high quality photo" (without CFG).
      </p>

      <figure>
        <img src="media/proj5_1_5.png" alt="Sampling without CFG">
        <figcaption>
          5 samples generated from pure Gaussian noise. Quality is moderate without classifier-free guidance.
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.6 ========= -->
    <section class="card" id="1-6">
      <h2>1.6 — Classifier-Free Guidance (CFG)</h2>
      <p>
        Improving sample quality using CFG with guidance scale γ=7.
        Formula: <code>ε = ε_u + γ(ε_c - ε_u)</code> where ε_c is conditional (with prompt) 
        and ε_u is unconditional (empty prompt "").
      </p>

      <h3>CFG Algorithm</h3>
      <pre>
For each denoising step:
  ε_c = UNet(x_t, t, prompt_embeds)        # conditional
  ε_u = UNet(x_t, t, empty_prompt_embeds)  # unconditional
  ε = ε_u + γ(ε_c - ε_u)                  # γ=7
      </pre>

      <figure>
        <img src="media/proj5_1_6.png" alt="Sampling with CFG">
        <figcaption>
          5 samples with CFG (scale=7). Dramatically improved quality compared to Part 1.5.
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.7 ========= -->
    <section class="card" id="1-7">
      <h2>1.7 — Image-to-Image Translation (SDEdit)</h2>
      <p>
        Adding noise to real images at different levels and denoising with prompt "a high quality photo".
        Lower i_start values (more noise) allow more creative changes; higher values preserve more of the original.
      </p>

      <h3>Campanile Edits</h3>
      <figure>
        <img src="media/proj5_1_7_ex1.png" alt="Campanile SDEdit">
        <figcaption>
          Campanile edited at i_start=[1, 3, 5, 7, 10, 20]. Notice gradual preservation of original structure.
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 1 Edits</h3>
      <figure>
        <img src="media/proj5_1_7_ex2.png" alt="Building SDEdit">
        <figcaption>
          Building/stairs edited at i_start=[1, 3, 5, 7, 10, 20]
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 2 Edits</h3>
      <figure>
        <img src="media/proj5_1_7_ex3.png" alt="Waterfall SDEdit">
        <figcaption>
          Waterfall/mountain edited at i_start=[1, 3, 5, 7, 10, 20]
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.7.1 ========= -->
    <section class="card" id="1-7-1">
      <h2>1.7.1 — Editing Hand-Drawn and Web Images</h2>
      <p>
        Projecting non-realistic images (sketches, web images) onto the natural image manifold
        using the same SDEdit procedure.
      </p>

      <h3>Web Image Edits</h3>
      <div class="grid two">
        <figure>
          <img src="media/proj5_1_7_1_web.png" alt="Web original">
          <figcaption>Original web image</figcaption>
        </figure>
        <figure>
          <img src="media/proj5_1_7_1_web_output.png" alt="Web edited">
          <figcaption>Edited at i_start=[1, 3, 5, 7, 10, 20]</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:1rem">Hand-Drawn Image 1</h3>
      <div class="grid two">
        <figure>
          <img src="media/proj5_1_7_1_hand1.png" alt="Hand1 original">
          <figcaption>Original hand drawing</figcaption>
        </figure>
        <figure>
          <img src="media/proj5_1_7_1_hand1_output.png" alt="Hand1 edited">
          <figcaption>Edited at i_start=[1, 3, 5, 7, 10, 20]</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:1rem">Hand-Drawn Image 2</h3>
      <div class="grid two">
        <figure>
          <img src="media/proj5_1_7_1_hand2.png" alt="Hand2 original">
          <figcaption>Original hand drawing</figcaption>
        </figure>
        <figure>
          <img src="media/proj5_1_7_1_hand2_output.png" alt="Hand2 edited">
          <figcaption>Edited at i_start=[1, 3, 5, 7, 10, 20]</figcaption>
        </figure>
      </div>
    </section>

    <!-- ========= 1.7.2 ========= -->
    <section class="card" id="1-7-2">
      <h2>1.7.2 — Inpainting (RePaint Algorithm)</h2>
      <p>
        Filling masked regions while preserving unmasked areas using the RePaint algorithm:
        <code>x_t ← m·x_t + (1-m)·forward(x_orig, t)</code>
      </p>

      <h3>RePaint Algorithm</h3>
      <pre>
For each denoising step:
  1. Denoise normally: x_t → x_{t-1}
  2. Replace unmasked regions with original (noisy):
     x_t ← mask·x_t + (1-mask)·forward(x_original, t)
  
Where mask=1 means "inpaint this region"
      </pre>

      <h3>Campanile Inpainting</h3>
      <figure>
        <img src="media/proj5_1_7_2_campanile.png" alt="Campanile inpaint">
        <figcaption>
          Campanile: Original → Mask (white=inpaint region) → Inpainted result
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Inpainting 1</h3>
      <figure>
        <img src="media/proj5_1_7_2_custom1.png" alt="Custom1 inpaint">
        <figcaption>
          Building inpainting: Original → Mask → Inpainted result (center region replaced)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Inpainting 2</h3>
      <figure>
        <img src="media/proj5_1_7_2_custom2.png" alt="Custom2 inpaint">
        <figcaption>
          Waterfall/mountain inpainting: Original → Mask → Inpainted result (sky region replaced)
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.7.3 ========= -->
    <section class="card" id="1-7-3">
      <h2>1.7.3 — Text-Conditional Image-to-Image Translation</h2>
      <p>
        Combining SDEdit with custom text prompts to guide the editing process.
        Using prompt: "a lithograph of waterfalls"
      </p>

      <h3>Campanile Text-Guided Edits</h3>
      <figure>
        <img src="media/proj5_1_7_3_campanile.png" alt="Campanile text edit">
        <figcaption>
          Campanile edited with prompt "a lithograph of waterfalls" at i_start=[1, 3, 5, 7, 10, 20].
          Gradually transforms toward waterfall style while preserving tower structure.
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 1 Text-Guided Edits</h3>
      <figure>
        <img src="media/proj5_1_7_3_custom1.png" alt="Custom1 text edit">
        <figcaption>
          Building edited with "a lithograph of waterfalls" prompt
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 2 Text-Guided Edits</h3>
      <figure>
        <img src="media/proj5_1_7_3_custom2.png" alt="Custom2 text edit">
        <figcaption>
          Waterfall/mountain edited with "a lithograph of waterfalls" prompt
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.8 ========= -->
    <section class="card" id="1-8">
      <h2>1.8 — Visual Anagrams</h2>
      <p>
        Creating optical illusions that reveal different images when flipped upside down.
        Algorithm: <code>ε = (ε_1 + flip(ε_2)) / 2</code>
      </p>

      <h3>Visual Anagram Algorithm</h3>
      <pre>
For each denoising step:
  ε_1 = UNet(x_t, t, prompt_1)          # noise for prompt 1
  ε_2 = UNet(flip(x_t), t, prompt_2)    # noise for flipped image with prompt 2
  ε_2_flipped_back = flip(ε_2)          # flip noise back
  ε = (ε_1 + ε_2_flipped_back) / 2     # average both noise estimates
  x_t ← denoise_step(x_t, ε)            # denoise with averaged noise
      </pre>

      <h3>Anagram 1: "Old Man" ↔ "Campfire"</h3>
      <figure>
        <img src="media/proj5_1_8_ex1.png" alt="Anagram old man campfire">
        <figcaption>
          "an oil painting of an old man" (normal) → "an oil painting of people around a campfire" (flipped)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Anagram 2: "Waterfalls" ↔ "Skull"</h3>
      <figure>
        <img src="media/proj5_1_8_ex2.png" alt="Anagram waterfalls skull">
        <figcaption>
          "a lithograph of waterfalls" (normal) → "a lithograph of a skull" (flipped)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Anagram 3: "Dog" ↔ "Rocket Ship"</h3>
      <figure>
        <img src="media/proj5_1_8_ex3.png" alt="Anagram dog rocket">
        <figcaption>
          "a photo of a dog" (normal) → "a rocket ship" (flipped)
        </figcaption>
      </figure>
    </section>

    <!-- ========= 1.9 ========= -->
    <section class="card" id="1-9">
      <h2>1.9 — Hybrid Images with Diffusion</h2>
      <p>
        Creating hybrid images by combining low frequencies from one prompt with high frequencies from another.
        Algorithm: <code>ε = f_lowpass(ε_1) + f_highpass(ε_2)</code>
      </p>

      <h3>Hybrid Image Algorithm</h3>
      <pre>
For each denoising step:
  ε_1 = UNet(x_t, t, prompt_1)
  ε_2 = UNet(x_t, t, prompt_2)
  
  # Apply frequency filtering (kernel_size=33, sigma=2)
  ε_low = gaussian_blur(ε_1)
  ε_high = ε_2 - gaussian_blur(ε_2)
  
  ε = ε_low + ε_high
  x_t ← denoise_step(x_t, ε)
      </pre>

      <h3>Hybrid 1: Skull + Waterfalls</h3>
      <figure>
        <img src="media/proj5_1_9_ex1.png" alt="Hybrid skull waterfalls">
        <figcaption>
          Low freq: "skull" + High freq: "waterfalls". Close up shows waterfalls, far away shows skull.
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Hybrid 2: Old Man + Campfire</h3>
      <figure>
        <img src="media/proj5_1_9_ex2.png" alt="Hybrid old man campfire">
        <figcaption>
          Low freq: "old man" + High freq: "people around campfire"
        </figcaption>
      </figure>

      <p style="margin-top:.6rem; color:var(--muted)">
        <strong>Effect:</strong> View close up to see high-frequency details, step back to see low-frequency structure.
        Similar to Project 2 hybrid images but created entirely through diffusion!
      </p>
    </section>

    <footer>
      <div class="hr"></div>
      <p>
        <strong>What I Learned:</strong> The most fascinating aspect of diffusion models is their flexibility
        in creating optical illusions and hybrid images. By manipulating noise estimates—averaging predictions
        from flipped images or separating frequency components—we can create images that satisfy multiple
        constraints simultaneously. This demonstrates that diffusion models don't just memorize training data
        but learn a rich representation of the image manifold that can be guided in creative ways.
      </p>
      <div class="hr" style="margin-top:1rem"></div>
      <p>© <span id="year"></span> Raymond Wang • <a href="https://raymond23101.github.io">raymond23101.github.io</a></p>
    </footer>
  </div>
  <script>document.getElementById('year').textContent = new Date().getFullYear();</script>
</body>
</html>
