<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Project 5 — Diffusion Models (CS180)</title>
  <meta name="description" content="Part A: DeepFloyd diffusion. Part B: Training diffusion models from scratch on MNIST." />
  <style>
    :root{
      --bg:#f5f9ff; --ink:#0e1b2c; --muted:#5a6b82;
      --accent:#3b82f6; --accent-2:#60a5fa;
      --card:#ffffff; --border:#e6edf7;
      --shadow:0 10px 25px rgba(25,74,142,.12);
      --radius:18px; --max:1080px;
    }
    html,body{background:var(--bg); color:var(--ink); font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, "Helvetica Neue", Arial}
    body{margin:0; padding:28px 18px 56px}
    .wrap{max-width:var(--max); margin:0 auto}
    a{color:var(--accent); text-decoration:none}
    header{margin-bottom:1rem}
    h1{font-size:clamp(1.6rem, 1.15rem + 2vw, 2.4rem); line-height:1.15; margin:.25rem 0}
    .pill{display:inline-block; padding:.25rem .6rem; border-radius:999px; border:1px solid rgba(59,130,246,.25); background:rgba(59,130,246,.12); color:#0b3a7a; font-weight:600; font-size:.85rem}

    .card{background:var(--card); border:1px solid var(--border); border-radius:var(--radius); padding:18px; box-shadow:var(--shadow); margin:18px 0}
    .grid{display:grid; gap:14px}
    .two{grid-template-columns:1fr}
    .three{grid-template-columns:1fr}
    .four{grid-template-columns:1fr}
    @media(min-width:780px){ 
      .two{grid-template-columns:1fr 1fr} 
      .three{grid-template-columns:1fr 1fr 1fr}
      .four{grid-template-columns:repeat(4, 1fr)}
    }

    figure{margin:0}
    img{width:100%; height:auto; display:block; border-radius:14px; border:1px solid var(--border)}
    figcaption{margin-top:.4rem; color:var(--muted); font-size:.95rem}
    h2{margin:.25rem 0 .65rem; font-size:1.3rem}
    h3{margin:.25rem 0 .4rem; font-size:1.05rem}
    .hr{height:1px; background:var(--border); margin:1.5rem 0}
    footer{margin-top:2rem; color:var(--muted); font-size:.95rem}
    pre{background:#f3f6ff; border:1px solid #e6edf7; padding:.6rem .75rem; border-radius:10px; overflow:auto; font-size:.92rem; line-height:1.35}
    code{background:#eef4ff; border:1px solid #d7e5ff; padding:.1rem .25rem; border-radius:6px}
    .part-header{background:linear-gradient(135deg, #3b82f6 0%, #60a5fa 100%); color:#fff; padding:1rem 1.25rem; border-radius:var(--radius); margin:2rem 0 1rem}
    .part-header h2{color:#fff; margin:0}
    .major-part{background:linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%); color:#fff; padding:1.5rem 1.25rem; border-radius:var(--radius); margin:3rem 0 1.5rem; font-size:1.4rem}
    .major-part h2{color:#fff; margin:0; font-size:1.6rem}
    @media print{ body{padding:0; background:#fff} .card{box-shadow:none} a[href]:after{content:""} }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <a href="/">← Back to Home</a>
      <h1>Programming Project #5 — Diffusion Models</h1>
      <span class="pill">CS180/280A</span>
      <p style="margin:.4rem 0 0; color:var(--muted)">
        <strong>Part A:</strong> Using pretrained DeepFloyd diffusion models.
        <strong>Part B:</strong> Training diffusion models from scratch on MNIST.
      </p>
    </header>

    <!-- ==================== PART A ==================== -->
    <div class="major-part"><h2>Part A — The Power of Diffusion Models</h2></div>

    <div class="part-header"><h2>Part 0 — Setup & Text Prompts</h2></div>

    <section class="card" id="part0">
      <h2>Generated Images from Text Prompts</h2>
      <p>
        Using DeepFloyd IF to generate images from custom text prompts. Testing with different
        <code>num_inference_steps</code> values to observe quality differences.
      </p>

      <figure>
        <img src="media/proj5_0.1.png" alt="Generated samples">
        <figcaption>
          Three generated samples: "an oil painting of a snowy mountain village", 
          "a lithograph of waterfalls", and "a rocket ship"
        </figcaption>
      </figure>

      <p style="margin-top:.8rem; color:var(--muted)">
        <strong>Random seed:</strong> 180<br>
        <strong>num_inference_steps tested:</strong> 20, 50
      </p>
    </section>

    <div class="part-header"><h2>Part 1 — Sampling Loops</h2></div>

    <section class="card" id="1-1">
      <h2>1.1 — Implementing the Forward Process</h2>
      <p>
        The forward diffusion process adds Gaussian noise to clean images according to:
        <code>x_t = √(ᾱ_t)·x_0 + √(1-ᾱ_t)·ε</code> where <code>ε ~ N(0, I)</code>.
      </p>

      <h3>Forward Process Equation</h3>
      <pre>
x_t = √(ᾱ_t) · x_0 + √(1-ᾱ_t) · ε

where:
  x_0 = clean image (Campanile)
  x_t = noisy image at timestep t
  ᾱ_t = noise schedule coefficient (alphas_cumprod[t])
  ε ~ N(0, I) = Gaussian noise
      </pre>

      <figure>
        <img src="media/proj5_1.1.png" alt="Forward process">
        <figcaption>
          Berkeley Campanile at different noise levels: Original, t=250, t=500, t=750
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-2">
      <h2>1.2 — Classical Denoising with Gaussian Blur</h2>
      <p>
        Attempting to denoise using classical Gaussian blur filtering (kernel_size=9, sigma=2.0).
        Results demonstrate the limitations of traditional methods compared to learned diffusion models.
      </p>

      <figure>
        <img src="media/proj5_1.2.png" alt="Gaussian blur denoising">
        <figcaption>
          Top row: Noisy images at t=250, 500, 750<br>
          Bottom row: Gaussian blur denoised results (limited effectiveness)
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-3">
      <h2>1.3 — One-Step Denoising</h2>
      <p>
        Using the pretrained UNet to estimate and remove noise in a single step:
        <code>x_0 = (x_t - √(1-ᾱ_t)·ε_θ(x_t, t)) / √(ᾱ_t)</code>
      </p>

      <figure>
        <img src="media/proj5_1.3.png" alt="One-step denoising">
        <figcaption>
          One-step denoising at t=250, 500, 750. Each triplet shows: Original, Noisy, One-step Denoised.
          Performance degrades at higher noise levels.
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-4">
      <h2>1.4 — Iterative Denoising (DDPM)</h2>
      <p>
        Implementing the full iterative denoising process with strided timesteps: [990, 960, ..., 30, 0].
        Starting from i_start=10 (t=690), we progressively remove noise over multiple steps.
      </p>

      <h3>DDPM Algorithm</h3>
      <pre>
strided_timesteps = [990, 960, 930, ..., 30, 0]  (stride=30)

For each step i → i+1:
  1. Estimate noise: ε = UNet(x_t, t, prompt_embeds)
  2. Estimate clean image: x_0 = (x_t - √(1-ᾱ_t)·ε) / √(ᾱ_t)
  3. Step to next timestep using DDPM formula
  4. Add variance for stochasticity
      </pre>

      <figure>
        <img src="media/proj5_1.4.png" alt="Iterative denoising">
        <figcaption>
          Comparison of denoising methods: Original → Noisy (t=690) → Iterative DDPM → 
          One-step → Gaussian Blur. Iterative denoising produces the best results.
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-5">
      <h2>1.5 — Diffusion Model Sampling</h2>
      <p>
        Generating images from pure noise by running iterative denoising from t=990 to t=0.
        Using prompt: "a high quality photo" (without CFG).
      </p>

      <figure>
        <img src="media/proj5_1.5.png" alt="Sampling without CFG">
        <figcaption>
          5 samples generated from pure Gaussian noise. Quality is moderate without classifier-free guidance.
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-6">
      <h2>1.6 — Classifier-Free Guidance (CFG)</h2>
      <p>
        Improving sample quality using CFG with guidance scale γ=7.
        Formula: <code>ε = ε_u + γ(ε_c - ε_u)</code> where ε_c is conditional (with prompt) 
        and ε_u is unconditional (empty prompt "").
      </p>

      <h3>CFG Algorithm</h3>
      <pre>
For each denoising step:
  ε_c = UNet(x_t, t, prompt_embeds)        # conditional
  ε_u = UNet(x_t, t, empty_prompt_embeds)  # unconditional
  ε = ε_u + γ(ε_c - ε_u)                  # γ=7
      </pre>

      <figure>
        <img src="media/proj5_1.6.png" alt="Sampling with CFG">
        <figcaption>
          5 samples with CFG (scale=7). Dramatically improved quality compared to Part 1.5.
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-7">
      <h2>1.7 — Image-to-Image Translation (SDEdit)</h2>
      <p>
        Adding noise to real images at different levels and denoising with prompt "a high quality photo".
        Lower i_start values (more noise) allow more creative changes; higher values preserve more of the original.
      </p>

      <h3>Campanile Edits</h3>
      <figure>
        <img src="media/proj5_1.7_ex1.png" alt="Campanile SDEdit">
        <figcaption>
          Campanile edited at i_start=[1, 3, 5, 7, 10, 20]. Notice gradual preservation of original structure.
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 1 Edits</h3>
      <figure>
        <img src="media/proj5_1.7_ex2.png" alt="Building SDEdit">
        <figcaption>
          Building/stairs edited at i_start=[1, 3, 5, 7, 10, 20]
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 2 Edits</h3>
      <figure>
        <img src="media/proj5_1.7_ex3.png" alt="Waterfall SDEdit">
        <figcaption>
          Waterfall/mountain edited at i_start=[1, 3, 5, 7, 10, 20]
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-7-1">
      <h2>1.7.1 — Editing Hand-Drawn and Web Images</h2>
      <p>
        Projecting non-realistic images (sketches, web images) onto the natural image manifold
        using the same SDEdit procedure.
      </p>

      <h3>Web Image Edits</h3>
      <div class="grid two">
        <figure>
          <img src="media/proj5_1.7.1_web.png" alt="Web original">
          <figcaption>Original web image</figcaption>
        </figure>
        <figure>
          <img src="media/proj5_1.7.1_web_output.png" alt="Web edited">
          <figcaption>Edited at i_start=[1, 3, 5, 7, 10, 20]</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:1rem">Hand-Drawn Image 1</h3>
      <div class="grid two">
        <figure>
          <img src="media/proj5_1.7.1_hand1.png" alt="Hand1 original">
          <figcaption>Original hand drawing</figcaption>
        </figure>
        <figure>
          <img src="media/proj5_1.7.1_hand1_output.png" alt="Hand1 edited">
          <figcaption>Edited at i_start=[1, 3, 5, 7, 10, 20]</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:1rem">Hand-Drawn Image 2</h3>
      <div class="grid two">
        <figure>
          <img src="media/proj5_1.7.1_hand2.png" alt="Hand2 original">
          <figcaption>Original hand drawing</figcaption>
        </figure>
        <figure>
          <img src="media/proj5_1.7.1_hand2_output.png" alt="Hand2 edited">
          <figcaption>Edited at i_start=[1, 3, 5, 7, 10, 20]</figcaption>
        </figure>
      </div>
    </section>

    <section class="card" id="1-7-2">
      <h2>1.7.2 — Inpainting (RePaint Algorithm)</h2>
      <p>
        Filling masked regions while preserving unmasked areas using the RePaint algorithm:
        <code>x_t ← m·x_t + (1-m)·forward(x_orig, t)</code>
      </p>

      <h3>RePaint Algorithm</h3>
      <pre>
For each denoising step:
  1. Denoise normally: x_t → x_{t-1}
  2. Replace unmasked regions with original (noisy):
     x_t ← mask·x_t + (1-mask)·forward(x_original, t)
  
Where mask=1 means "inpaint this region"
      </pre>

      <h3>Campanile Inpainting</h3>
      <figure>
        <img src="media/proj5_1.7.2_campanile.png" alt="Campanile inpaint">
        <figcaption>
          Campanile: Original → Mask (white=inpaint region) → Inpainted result
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Inpainting 1</h3>
      <figure>
        <img src="media/proj5_1.7.2_custom1.png" alt="Custom1 inpaint">
        <figcaption>
          Building inpainting: Original → Mask → Inpainted result (center region replaced)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Inpainting 2</h3>
      <figure>
        <img src="media/proj5_1.7.2_custom2.png" alt="Custom2 inpaint">
        <figcaption>
          Waterfall/mountain inpainting: Original → Mask → Inpainted result (sky region replaced)
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-7-3">
      <h2>1.7.3 — Text-Conditional Image-to-Image Translation</h2>
      <p>
        Combining SDEdit with custom text prompts to guide the editing process.
        Using prompt: "a lithograph of waterfalls"
      </p>

      <h3>Campanile Text-Guided Edits</h3>
      <figure>
        <img src="media/proj5_1.7.3_campanile.png" alt="Campanile text edit">
        <figcaption>
          Campanile edited with prompt "a lithograph of waterfalls" at i_start=[1, 3, 5, 7, 10, 20].
          Gradually transforms toward waterfall style while preserving tower structure.
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 1 Text-Guided Edits</h3>
      <figure>
        <img src="media/proj5_1.7.3_custom1.png" alt="Custom1 text edit">
        <figcaption>
          Building edited with "a lithograph of waterfalls" prompt
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Custom Image 2 Text-Guided Edits</h3>
      <figure>
        <img src="media/proj5_1.7.3_custom2.png" alt="Custom2 text edit">
        <figcaption>
          Waterfall/mountain edited with "a lithograph of waterfalls" prompt
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-8">
      <h2>1.8 — Visual Anagrams</h2>
      <p>
        Creating optical illusions that reveal different images when flipped upside down.
        Algorithm: <code>ε = (ε_1 + flip(ε_2)) / 2</code>
      </p>

      <h3>Visual Anagram Algorithm</h3>
      <pre>
For each denoising step:
  ε_1 = UNet(x_t, t, prompt_1)          # noise for prompt 1
  ε_2 = UNet(flip(x_t), t, prompt_2)    # noise for flipped image with prompt 2
  ε_2_flipped_back = flip(ε_2)          # flip noise back
  ε = (ε_1 + ε_2_flipped_back) / 2     # average both noise estimates
  x_t ← denoise_step(x_t, ε)            # denoise with averaged noise
      </pre>

      <h3>Anagram 1: "Old Man" ↔ "Campfire"</h3>
      <figure>
        <img src="media/proj5_1.8_ex1.png" alt="Anagram old man campfire">
        <figcaption>
          "an oil painting of an old man" (normal) → "an oil painting of people around a campfire" (flipped)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Anagram 2: "Waterfalls" ↔ "Skull"</h3>
      <figure>
        <img src="media/proj5_1.8_ex2.png" alt="Anagram waterfalls skull">
        <figcaption>
          "a lithograph of waterfalls" (normal) → "a lithograph of a skull" (flipped)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Anagram 3: "Dog" ↔ "Rocket Ship"</h3>
      <figure>
        <img src="media/proj5_1.8_ex3.png" alt="Anagram dog rocket">
        <figcaption>
          "a photo of a dog" (normal) → "a rocket ship" (flipped)
        </figcaption>
      </figure>
    </section>

    <section class="card" id="1-9">
      <h2>1.9 — Hybrid Images with Diffusion</h2>
      <p>
        Creating hybrid images by combining low frequencies from one prompt with high frequencies from another.
        Algorithm: <code>ε = f_lowpass(ε_1) + f_highpass(ε_2)</code>
      </p>

      <h3>Hybrid Image Algorithm</h3>
      <pre>
For each denoising step:
  ε_1 = UNet(x_t, t, prompt_1)
  ε_2 = UNet(x_t, t, prompt_2)
  
  # Apply frequency filtering (kernel_size=33, sigma=2)
  ε_low = gaussian_blur(ε_1)
  ε_high = ε_2 - gaussian_blur(ε_2)
  
  ε = ε_low + ε_high
  x_t ← denoise_step(x_t, ε)
      </pre>

      <h3>Hybrid 1: Skull + Waterfalls</h3>
      <figure>
        <img src="media/proj5_1.9_ex1.png" alt="Hybrid skull waterfalls">
        <figcaption>
          Low freq: "skull" + High freq: "waterfalls". Close up shows waterfalls, far away shows skull.
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Hybrid 2: Old Man + Campfire</h3>
      <figure>
        <img src="media/proj5_1.9_ex2.png" alt="Hybrid old man campfire">
        <figcaption>
          Low freq: "old man" + High freq: "people around campfire"
        </figcaption>
      </figure>

      <p style="margin-top:.6rem; color:var(--muted)">
        <strong>Effect:</strong> View close up to see high-frequency details, step back to see low-frequency structure.
        Similar to Project 2 hybrid images but created entirely through diffusion!
      </p>
    </section>

    <!-- ==================== PART B ==================== -->
    <div class="major-part"><h2>Part B — Training Diffusion Models from Scratch</h2></div>

    <div class="part-header"><h2>Part 1 — Training a Single-Step Denoising UNet</h2></div>

    <section class="card" id="b-1-1">
      <h2>1.1 — Implementing the UNet</h2>
      <p>
        Implementing a UNet architecture with encoder-decoder structure and skip connections.
        The network consists of downsampling blocks, a bottleneck, and upsampling blocks with skip connections.
      </p>

      <h3>UNet Architecture</h3>
      <pre>
Encoder:
  ConvBlock(1 → D)  @ 28×28
  DownBlock(D → D)  @ 14×14  [skip1]
  DownBlock(D → 2D) @ 7×7    [skip2]

Bottleneck:
  Flatten  (7×7 → 1×1)
  Unflatten (1×1 → 7×7)

Decoder:
  UpBlock(2D → D) @ 14×14  + skip2
  UpBlock(D → D)  @ 28×28  + skip1
  
Output:
  ConvBlock(D → D) + Conv2d(D → 1)

Components:
- Conv: Conv2d + BatchNorm + GELU
- DownConv: Conv2d(stride=2) + BatchNorm + GELU
- UpConv: ConvTranspose2d + BatchNorm + GELU
- ConvBlock: Conv → Conv (two sequential)
- DownBlock: DownConv → ConvBlock
- UpBlock: UpConv → Concat(skip) → ConvBlock
      </pre>
    </section>

    <section class="card" id="b-1-2-1">
      <h2>1.2.1 — Training the Denoiser (σ=0.5)</h2>
      <p>
        Training a UNet to denoise MNIST digits corrupted with Gaussian noise at σ=0.5.
        <strong>Hyperparameters:</strong> D=128, batch_size=256, lr=1e-4, epochs=5
      </p>

      <h3>Noising Process Visualization</h3>
      <figure>
        <img src="media/proj5b_1.2.png" alt="Noising process">
        <figcaption>
          MNIST digit with varying noise levels: σ = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Training Loss Curve</h3>
      <figure>
        <img src="media/proj5b_1.2.1_loss_graphs.jpg" alt="Training loss">
        <figcaption>
          Training loss over 5 epochs (σ=0.5)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Denoising Results</h3>
      <figure>
        <img src="media/proj5b_1.2.1_epochs.png" alt="Epoch results">
        <figcaption>Results after Epoch 5 (clearer than Epoch 1)</figcaption>
      </figure>
    </section>

    <section class="card" id="b-1-2-2">
      <h2>1.2.2 — Out-of-Distribution Testing</h2>
      <p>
        Testing the denoiser (trained on σ=0.5) on different noise levels to evaluate generalization.
      </p>

      <figure>
        <img src="media/proj5b_1.2.2.png" alt="OOD testing">
        <figcaption>
          Out-of-distribution testing: Model performs best at σ=0.5 (training level), degrades for other values
        </figcaption>
      </figure>

      <p style="margin-top:.6rem; color:var(--muted)">
        <strong>Observation:</strong> Performance is optimal at σ=0.5 (training noise level) and degrades 
        for noise levels far from 0.5, demonstrating limited out-of-distribution generalization.
      </p>
    </section>

    <section class="card" id="b-1-2-3">
      <h2>1.2.3 — Denoising Pure Noise</h2>
      <p>
        Training to denoise pure random noise z ~ N(0,I) to clean MNIST digits.
        With MSE loss, the model learns to output the centroid of the training distribution.
      </p>

      <h3>Training Loss</h3>
      <figure>
        <img src="media/proj5b_1.2.3_loss_graphs.png" alt="Pure noise loss">
        <figcaption>Training loss for pure noise denoising</figcaption>
      </figure>

      <h3 style="margin-top:1rem">Results</h3>
      <figure>
        <img src="media/proj5b_1.2.3_epochs.png" alt="Pure noise results">
        <figcaption>Epoch 1 (top) and Epoch 5 (bottom): Converges to "average digit"</figcaption>
      </figure>

      <h3 style="margin-top:1rem">Analysis</h3>
      <pre>
OBSERVED PATTERNS:
The model outputs a blurry "average digit" that doesn't clearly 
represent any specific digit (0-9). All generated samples look 
very similar regardless of the input noise.

EXPLANATION:
Since the input is pure random noise with no information about 
which digit to generate, and we're using MSE loss, the optimal 
solution is to output E[x] = the mean of all training images.

MSE loss minimizes: L = E[||D(z) - x||²]
Optimal prediction: D(z) = E[x] = average of all MNIST digits

This is analogous to finding the centroid in clustering - the 
point that minimizes sum of squared distances to all data points.
The model learns this "average digit" as it's the best it can do 
when given no information about the target class.
      </pre>
    </section>

    <div class="part-header"><h2>Part 2 — Training a Flow Matching Model</h2></div>

    <section class="card" id="b-2-1">
      <h2>2.1 — Adding Time Conditioning to UNet</h2>
      <p>
        Injecting scalar timestep t ∈ [0,1] into the UNet using FCBlocks (fully-connected blocks).
        The time embedding modulates features after the bottleneck and first upsampling.
      </p>

      <h3>Time Conditioning Architecture</h3>
      <pre>
FCBlock: Linear(F_in, F_out) → GELU → Linear(F_out, F_out)

Time Embedding:
  fc1_t = FCBlock(1, 2D)  # t is scalar → 2D features
  fc2_t = FCBlock(1, D)   # t is scalar → D features
  
  t1 = fc1_t(t)
  t2 = fc2_t(t)

Modulation (element-wise multiplication):
  unflatten = unflatten * t1
  up1 = up1 * t2

Note: t must be normalized to [0, 1] before embedding
      </pre>
    </section>

    <section class="card" id="b-2-2">
      <h2>2.2 — Training the Time-Conditioned UNet</h2>
      <p>
        Training using flow matching objective. For each batch: sample random t ~ Uniform(0,1),
        interpolate x_t = (1-t)·x_1 + t·x_0, predict flow v = x_0 - x_1.
        <strong>Hyperparameters:</strong> D=64, batch_size=64, lr=1e-2, ExponentialLR decay
      </p>

      <h3>Flow Matching Algorithm</h3>
      <pre>
For each training step:
  1. Sample clean image x_1 from MNIST
  2. Sample noise x_0 ~ N(0, I)
  3. Sample timestep t ~ Uniform(0, 1)
  4. Interpolate: x_t = (1-t)·x_1 + t·x_0
  5. Target flow: v = x_0 - x_1
  6. Predicted flow: v_pred = UNet(x_t, t)
  7. Loss: MSE(v_pred, v)
      </pre>

      <figure>
        <img src="media/proj5b_2.2.png" alt="Time-conditional loss">
        <figcaption>Training loss for time-conditioned UNet (5 epochs)</figcaption>
      </figure>
    </section>

    <section class="card" id="b-2-3">
      <h2>2.3 — Sampling from Time-Conditioned UNet</h2>
      <p>
        Using Euler method to iteratively denoise from pure noise (t=1) to clean image (t=0).
      </p>

      <h3>Sampling Algorithm</h3>
      <pre>
Initialize: x = random noise ~ N(0, I)
Timesteps: t = [1.0, 0.99, 0.98, ..., 0.01, 0.0]

For each step:
  dt = t[i] - t[i+1]
  v = UNet(x, t[i])
  x = x - dt * v

Return: x (generated digit)
      </pre>

      <div class="grid three">
        <figure>
          <img src="media/proj5b_2.3_epoch1.png" alt="Epoch 1 samples">
          <figcaption>Epoch 1: Mostly noise</figcaption>
        </figure>
        <figure>
          <img src="media/proj5b_2.3_epoch2.png" alt="Epoch 5 samples">
          <figcaption>Epoch 5: Recognizable digits</figcaption>
        </figure>
        <figure>
          <img src="media/proj5b_2.3_epoch3.png" alt="Epoch 10 samples">
          <figcaption>Epoch 10: Clear digits</figcaption>
        </figure>
      </div>
    </section>

    <section class="card" id="b-2-4">
      <h2>2.4 — Adding Class-Conditioning to UNet</h2>
      <p>
        Conditioning on digit class (0-9) using one-hot vectors with 10% dropout for classifier-free guidance.
      </p>

      <h3>Class Conditioning Architecture</h3>
      <pre>
FCBlock for class:
  fc1_c = FCBlock(10, 2D)  # one-hot(10) → 2D features
  fc2_c = FCBlock(10, D)   # one-hot(10) → D features

Dropout (p_uncond = 0.1):
  With 10% probability: set c = zeros(10)
  Otherwise: use one_hot(class_label, num_classes=10)

Combined modulation:
  unflatten = c1 * unflatten + t1
  up1 = c2 * up1 + t2

where c1, c2 are class embeddings and t1, t2 are time embeddings
      </pre>
    </section>

    <section class="card" id="b-2-5">
      <h2>2.5 — Training Class-Conditioned UNet</h2>
      <p>
        Training with both time and class conditioning. Same as 2.2 but with class labels and dropout.
      </p>

      <figure>
        <img src="media/proj5b_2.5.png" alt="Class-conditional loss">
        <figcaption>Training loss for class-conditioned UNet (10 epochs, no scheduler)</figcaption>
      </figure>
    </section>

    <section class="card" id="b-2-6">
      <h2>2.6 — Sampling with Classifier-Free Guidance</h2>
      <p>
        Sampling with CFG scale γ=5.0. For each step, predict both conditional and unconditional flow,
        then extrapolate: v = v_uncond + γ(v_cond - v_uncond)
      </p>

      <h3>CFG Sampling Algorithm</h3>
      <pre>
For each denoising step:
  v_cond = UNet(x, t, class_label)      # with class
  v_uncond = UNet(x, t, zeros(10))      # without class
  v_cfg = v_uncond + γ(v_cond - v_uncond)  # γ=5.0
  x = x - dt * v_cfg
      </pre>

      <div class="grid three">
        <figure>
          <img src="media/proj5b_2.6_epoch1.png" alt="Epoch 1 class samples">
          <figcaption>Epoch 1: Noisy</figcaption>
        </figure>
        <figure>
          <img src="media/proj5b_2.6_epoch2.png" alt="Epoch 5 class samples">
          <figcaption>Epoch 5: Improving</figcaption>
        </figure>
        <figure>
          <img src="media/proj5b_2.6_epoch3.png" alt="Epoch 10 class samples">
          <figcaption>Epoch 10: High quality, 4 instances per digit 0-9</figcaption>
        </figure>
      </div>

      <div class="hr"></div>

      <h3>Training Without Learning Rate Scheduler</h3>
      <p>
        Removing the ExponentialLR scheduler while maintaining performance by using a constant 
        learning rate of 1e-2 throughout all 10 epochs.
      </p>

      <figure>
        <img src="media/proj5b_2.6_loss.png" alt="No scheduler loss">
        <figcaption>
          Training loss without scheduler (constant lr=1e-2)
        </figcaption>
      </figure>

      <h3 style="margin-top:1rem">Compensation Strategy</h3>
      <pre>
WITHOUT Scheduler:
- Used constant lr=1e-2 throughout all 10 epochs
- Simpler training loop (no scheduler.step())
- Similar final performance to scheduler version

WHY IT WORKS:
1. Flow matching is robust to learning rate schedules
2. Adam's adaptive learning rates provide stability
3. Sufficient epochs (10) allow convergence even with constant LR
4. CFG at sampling (γ=5.0) compensates for any training deficiencies

TRADE-OFFS:
- Simpler code and fewer hyperparameters
- May have slightly more oscillation in later epochs
- Comparable final sample quality to scheduler version

RESULT: The exponential LR scheduler is helpful but not critical.
Constant learning rate with proper hyperparameters achieves 
comparable results, demonstrating the robustness of flow matching.
      </pre>
    </section>

    <footer>
      <div class="hr"></div>
      <p>
        <strong>Most Interesting Thing Learned:</strong> The power of diffusion models extends beyond 
        simple generation. Part A showed how manipulating noise estimates enables optical illusions 
        (visual anagrams) and hybrid images. Part B demonstrated that flow matching provides an elegant 
        alternative to traditional DDPM, using direct flow prediction rather than noise estimation. 
        The combination of classifier-free guidance with proper conditioning allows control over 
        generation while maintaining high quality—all from models we can train from scratch!
      </p>
      <div class="hr" style="margin-top:1rem"></div>
      <p>© <span id="year"></span> Raymond Wang • <a href="https://raymond23101.github.io">raymond23101.github.io</a></p>
    </footer>
  </div>
  <script>document.getElementById('year').textContent = new Date().getFullYear();</script>
</body>
</html>
